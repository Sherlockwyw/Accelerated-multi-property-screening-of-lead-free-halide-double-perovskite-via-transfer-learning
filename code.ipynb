{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30fcb9-96b9-45ef-acf6-4488d7d0b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions needed for model building\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from aquarel import load_theme\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "\n",
    "# 生成基模型\n",
    "def getBaseModel(X_train,y_train,X_test,y_test,unit,epochs=120,learning_rate=0.00005,batch_size=32,k=10,fig=True):\n",
    "    '''\n",
    "    训练返回基模型\n",
    "    '''\n",
    "    x_train = X_train\n",
    "    y_train = y_train\n",
    "    x_test = X_test\n",
    "    y_test = y_test\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    sklearn_model = build_model(x_test_scaled,learning_rate=learning_rate)\n",
    "    sklearn_model.summary()\n",
    "    \n",
    "    history = sklearn_model.fit(x_train_scaled, y_train,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test_scaled, y_test),\n",
    "#                             batch_size=batch_size,   \n",
    "                            verbose=0)\n",
    "    # 绘制学习曲线\n",
    "    print(history.params)\n",
    "    print_history(history) #调用绘图函数\n",
    "    \n",
    "    y_test_pred = sklearn_model.predict(x_test_scaled)\n",
    "    y_train_pred = sklearn_model.predict(x_train_scaled)\n",
    "\n",
    "    print(f'Train_rmse: {mean_squared_error(y_train_pred, y_train,squared=False)}')\n",
    "    print(f'Train_R2: {r2_score(y_train_pred, y_train)}') \n",
    "    print(f'Test_rmse: {mean_squared_error(y_test_pred, y_test,squared=False)}')\n",
    "    print(f'Test_R2: {r2_score(y_test_pred, y_test)}')\n",
    "    print(f'Train Dataset: {len(x_train_scaled)}')\n",
    "    if fig:\n",
    "        figAccScatter(y_test,y_test_pred,unit=unit)\n",
    "\n",
    "    return sklearn_model\n",
    "\n",
    "def ANN_DL_byCV(X,y,unit,k=10,patience=80,learning_rate=0.00005,fig=True):\n",
    "    '''\n",
    "    ANN 交叉验证直接训练\n",
    "    '''\n",
    "    #交叉验证训练\n",
    "    kfold =  KFold(n_splits=k, shuffle=True\n",
    "                      , random_state=1234\n",
    "                     )\n",
    "    \n",
    "    y_test_pred_list,y_test_true_list = [],[]\n",
    "    y_train_rmse_list,y_train_r2_list,y_test_rmse_list,y_test_r2_list = [],[],[],[]\n",
    "    count = 0\n",
    "    for train_i, test_i in kfold.split(X):\n",
    "        count+=1\n",
    "        print(f'-------================== LOOP {count}/{k} ===================-------')\n",
    "        x_train = X.iloc[train_i,:]\n",
    "        y_train = y[train_i]\n",
    "        x_test = X.iloc[test_i,:]\n",
    "        y_test = y[test_i]\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(x_train)\n",
    "        x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "        sklearn_model = build_model(x_test_scaled,learning_rate=learning_rate)\n",
    "        sklearn_model.summary()\n",
    "        callbacks = [keras.callbacks.EarlyStopping(patience=patience, min_delta=1e-2)]\n",
    "\n",
    "        history = sklearn_model.fit(x_train_scaled, y_train,\n",
    "                                epochs=1000,\n",
    "                                validation_data=(x_test_scaled, y_test),\n",
    "                                callbacks=callbacks,\n",
    "                                   verbose=0)\n",
    "                # 绘制学习曲线\n",
    "        print(history.params)\n",
    "        print_history(history) #调用绘图函数\n",
    "    \n",
    "        y_test_pred = sklearn_model.predict(x_test_scaled)\n",
    "        y_train_pred = sklearn_model.predict(x_train_scaled)\n",
    "        y_test_pred_list.append(y_test_pred)\n",
    "        y_test_true_list.append(y_test)\n",
    "        \n",
    "        y_train_rmse_list.append(mean_squared_error(y_train_pred, y_train,squared=False))\n",
    "        y_train_r2_list.append(r2_score(y_train_pred, y_train)) \n",
    "    y_test_pred_list=list(itertools.chain(*y_test_pred_list))\n",
    "    y_test_true_list=list(itertools.chain(*y_test_true_list))\n",
    "    test_rmse = mean_squared_error(y_test_true_list, y_test_pred_list,squared=False)\n",
    "    test_R2 = r2_score(y_test_true_list, y_test_pred_list)\n",
    "    train_rmse = np.mean(y_train_rmse_list)\n",
    "    train_R2 = np.mean(y_train_r2_list)\n",
    "    print('Train rmse: ',train_rmse)\n",
    "    print('Train R2: ',train_R2)\n",
    "    print('Test rmse: ',test_rmse)\n",
    "    print('Test R2: ',test_R2)\n",
    "    if fig:\n",
    "        figAccScatter(y_test_true_list,y_test_pred_list,unit=unit)\n",
    "    return [train_rmse,train_R2,test_rmse,test_R2]\n",
    "\n",
    "def ANN_DL_byLOO(X,y,unit,learning_rate=0.00005,patience=80,fig=True):\n",
    "    '''\n",
    "    用于ANN模型留一法直接训练\n",
    "    '''\n",
    "    loo = LeaveOneOut()\n",
    "    y_test_pred_list,y_test_true_list = [],[]\n",
    "    y_train_rmse_list,y_train_r2_list = [],[]\n",
    "    count=0\n",
    "    for train_i, test_i in loo.split(X):\n",
    "        count+=1\n",
    "        print(f'-------================== LOOP {count}/{len(y)} ===================-------')\n",
    "        x_train = X.iloc[train_i,:]\n",
    "        y_train = y[train_i]\n",
    "        x_test = X.iloc[test_i,:]\n",
    "        y_test = y[test_i]\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(x_train)\n",
    "        x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "        sklearn_model = build_model(x_test_scaled,learning_rate=learning_rate)\n",
    "        sklearn_model.summary()\n",
    "        callbacks = [keras.callbacks.EarlyStopping(patience=patience, min_delta=1e-2)]\n",
    "\n",
    "        history = sklearn_model.fit(x_train_scaled, y_train,\n",
    "                                epochs=1000,\n",
    "                                validation_data=(x_test_scaled, y_test),\n",
    "                                callbacks=callbacks,\n",
    "                                   verbose=0)\n",
    "                # 绘制学习曲线\n",
    "        print(history.params)\n",
    "        print_history(history) #调用绘图函数\n",
    "        \n",
    "        y_test_pred = sklearn_model.predict(x_test_scaled)\n",
    "        y_train_pred = sklearn_model.predict(x_train_scaled)\n",
    "        y_test_pred_list.append(y_test_pred)\n",
    "        y_test_true_list.append(y_test)\n",
    "        \n",
    "        y_train_rmse_list.append(mean_squared_error(y_train_pred, y_train,squared=False))\n",
    "        y_train_r2_list.append(r2_score(y_train_pred, y_train)) \n",
    "    y_test_pred_list=list(itertools.chain(*y_test_pred_list))\n",
    "    y_test_true_list=list(itertools.chain(*y_test_true_list))\n",
    "    test_rmse = mean_squared_error(y_test_true_list, y_test_pred_list,squared=False)\n",
    "    test_R2 = r2_score(y_test_true_list, y_test_pred_list)\n",
    "    train_rmse = np.mean(y_train_rmse_list)\n",
    "    train_R2 = np.mean(y_train_r2_list)\n",
    "    print('Train rmse: ',train_rmse)\n",
    "    print('Train R2: ',train_R2)\n",
    "    print('Test rmse: ',test_rmse)\n",
    "    print('Test R2: ',test_R2)\n",
    "    if fig:\n",
    "        figAccScatter(y_test_true_list,y_test_pred_list,unit=unit)\n",
    "    return [train_rmse,train_R2,test_rmse,test_R2]\n",
    "\n",
    "def ANN_DL_byTest(X_train,y_train,X_test,y_test,unit,epochs=120,learning_rate=0.00005,batch_size=32,fig=True):\n",
    "    '''\n",
    "    独立测试集验证ANN直接学习\n",
    "    '''\n",
    "    x_train = X_train\n",
    "    y_train = y_train\n",
    "    x_test = X_test\n",
    "    y_test = y_test\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    sklearn_model = build_model(x_test_scaled,learning_rate=learning_rate)\n",
    "    sklearn_model.summary()\n",
    "    callbacks = [keras.callbacks.EarlyStopping(patience=80, min_delta=1e-2)]\n",
    "\n",
    "    history = sklearn_model.fit(x_train_scaled, y_train,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test_scaled, y_test),\n",
    "#                             callbacks=callbacks,\n",
    "                               verbose=0)\n",
    "            # 绘制学习曲线\n",
    "    print(history.params)\n",
    "    print_history(history) #调用绘图函数\n",
    "    \n",
    "    y_test_pred = sklearn_model.predict(x_test_scaled)\n",
    "    y_train_pred = sklearn_model.predict(x_train_scaled)\n",
    "\n",
    "    print(f'Train_rmse: {mean_squared_error(y_train_pred, y_train,squared=False)}')\n",
    "    print(f'Train_R2: {r2_score(y_train_pred, y_train)}') \n",
    "    print(f'Test_rmse: {mean_squared_error(y_test_pred, y_test,squared=False)}')\n",
    "    print(f'Test_R2: {r2_score(y_test_pred, y_test)}')\n",
    "    if fig:\n",
    "        figAccScatter(y_test,y_test_pred,unit=unit)\n",
    "\n",
    "    return sklearn_model\n",
    "\n",
    "\n",
    "\n",
    "# 迁移学习后观察交叉验证结果\n",
    "def ANN_TL_byCV(X,y,importbaseModelPath,unit,patience=80,k=10,importbaseModel=None,learning_rate=0.00005,batch_size=32,fig=True):\n",
    "    '''\n",
    "    交叉验证迁移学习\n",
    "    '''\n",
    "    #交叉验证训练\n",
    "    kfold =  KFold(n_splits=k, shuffle=True\n",
    "                      , random_state=1234\n",
    "                     )\n",
    "    \n",
    "    y_test_pred_list,y_test_true_list = [],[]\n",
    "    y_train_rmse_list,y_train_r2_list,y_test_rmse_list,y_test_r2_list = [],[],[],[]\n",
    "    count = 0\n",
    "    for train_i, test_i in kfold.split(X):\n",
    "        count+=1\n",
    "        x_train = X.iloc[train_i,:]\n",
    "        y_train = y[train_i]\n",
    "        x_test = X.iloc[test_i,:]\n",
    "        y_test = y[test_i]\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(x_train)\n",
    "        x_test_scaled = scaler.transform(x_test)\n",
    "        #迁移模型\n",
    "        if importbaseModel == None:\n",
    "            print(f'---------========== learning by import baseModel file {count}/{k}============-----------')\n",
    "            sklearn_model=None\n",
    "            sklearn_model = build_model(x_test_scaled,learning_rate=learning_rate)\n",
    "            sklearn_model.load_weights(importbaseModelPath)\n",
    "            newl_last_1 = Dense(1, activation=None)(sklearn_model.layers[-2].output)  \n",
    "            sklearn_model = Model(inputs=sklearn_model.input, outputs=[newl_last_1])\n",
    "        else:\n",
    "            print(f'---------========== learning by import baseModel variables(object) {count}/{k}===========------------')\n",
    "            sklearn_model=None\n",
    "            newl_last_1 = Dense(1, activation=None)(importbaseModel.layers[-2].output)  \n",
    "            sklearn_model = Model(inputs=importbaseModel.input, outputs=[newl_last_1])\n",
    "        \n",
    "        sklearn_model.summary()\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        sklearn_model.compile(loss='mse', optimizer=optimizer,\n",
    "                              metrics=[keras.metrics.RootMeanSquaredError()]\n",
    "                             )\n",
    "        callbacks = [keras.callbacks.EarlyStopping(patience=patience, min_delta=1e-2)]\n",
    "\n",
    "        history = sklearn_model.fit(x_train_scaled, y_train,\n",
    "                                epochs=1000,\n",
    "#                                 batch_size=batch_size,\n",
    "                                validation_data=(x_test_scaled, y_test),\n",
    "                                callbacks=callbacks,\n",
    "                                   verbose=0)\n",
    "            # 绘制学习曲线\n",
    "        print(history.params)\n",
    "        print_history(history) #调用绘图函数\n",
    "        \n",
    "        y_test_pred = sklearn_model.predict(x_test_scaled)\n",
    "        y_train_pred = sklearn_model.predict(x_train_scaled)\n",
    "        y_test_pred_list.append(y_test_pred)\n",
    "        y_test_true_list.append(y_test)\n",
    "        \n",
    "        y_train_rmse_list.append(mean_squared_error(y_train_pred, y_train,squared=False))\n",
    "        y_train_r2_list.append(r2_score(y_train_pred, y_train)) \n",
    "    y_test_pred_list=list(itertools.chain(*y_test_pred_list))\n",
    "    y_test_true_list=list(itertools.chain(*y_test_true_list))\n",
    "    test_rmse = mean_squared_error(y_test_true_list, y_test_pred_list,squared=False)\n",
    "    test_R2 = r2_score(y_test_true_list, y_test_pred_list)\n",
    "    train_rmse = np.mean(y_train_rmse_list)\n",
    "    train_R2 = np.mean(y_train_r2_list)\n",
    "    print('Train rmse: ',train_rmse)\n",
    "    print('Train R2: ',train_R2)\n",
    "    print('Test rmse: ',test_rmse)\n",
    "    print('Test R2: ',test_R2)\n",
    "    if fig:\n",
    "        figAccScatter(y_test_true_list,y_test_pred_list,unit=unit)\n",
    "    return [train_rmse,train_R2,test_rmse,test_R2]\n",
    "\n",
    "#模型定义函数\n",
    "\n",
    "def ANN_TL_byLOO(X,y,importbaseModelPath,unit,patience=80,k=10,importbaseModel=None,learning_rate=0.00005,batch_size=32,fig=True):\n",
    "    '''\n",
    "    留一法验证迁移学习\n",
    "    '''\n",
    "    \n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    y_test_pred_list,y_test_true_list = [],[]\n",
    "    y_train_rmse_list,y_train_r2_list,y_test_rmse_list,y_test_r2_list = [],[],[],[]\n",
    "    count = 0\n",
    "    for train_i, test_i in loo.split(X):\n",
    "        count+=1\n",
    "        x_train = X.iloc[train_i,:]\n",
    "        y_train = y[train_i]\n",
    "        x_test = X.iloc[test_i,:]\n",
    "        y_test = y[test_i]\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(x_train)\n",
    "        x_test_scaled = scaler.transform(x_test)\n",
    "       \n",
    "        #迁移模型\n",
    "        if importbaseModel == None:\n",
    "            print(f'---------========== learning by import baseModel file {count}/{len(y)}============-----------')\n",
    "            sklearn_model=None\n",
    "            sklearn_model = build_model(x_test_scaled,learning_rate=learning_rate)\n",
    "            sklearn_model.load_weights(importbaseModelPath)\n",
    "            newl_last_1 = Dense(1, activation=None)(sklearn_model.layers[-2].output)  \n",
    "            sklearn_model = Model(inputs=sklearn_model.input, outputs=[newl_last_1])\n",
    "        else:\n",
    "            print(f'---------========== learning by import baseModel variables(object) {count}/{len(y)}===========------------')\n",
    "            sklearn_model=None\n",
    "            newl_last_1 = Dense(1, activation=None)(importbaseModel.layers[-2].output)  \n",
    "            sklearn_model = Model(inputs=importbaseModel.input, outputs=[newl_last_1])\n",
    "        \n",
    "        sklearn_model.summary()\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        sklearn_model.compile(loss='mse', optimizer=optimizer,metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        callbacks = [keras.callbacks.EarlyStopping(patience=patience, min_delta=1e-2)]\n",
    "\n",
    "        history = sklearn_model.fit(x_train_scaled, y_train,\n",
    "                                epochs=1000,\n",
    "                                validation_data=(x_test_scaled, y_test),\n",
    "#                                     batch_size=batch_size,\n",
    "                                callbacks=callbacks,\n",
    "                                   verbose=0)\n",
    "        # 绘制学习曲线\n",
    "        print(history.params)\n",
    "        print_history(history) #调用绘图函数\n",
    "        \n",
    "        y_test_pred = sklearn_model.predict(x_test_scaled)\n",
    "        y_train_pred = sklearn_model.predict(x_train_scaled)\n",
    "        y_test_pred_list.append(y_test_pred)\n",
    "        y_test_true_list.append(y_test)\n",
    "        \n",
    "        y_train_rmse_list.append(mean_squared_error(y_train_pred, y_train,squared=False))\n",
    "        y_train_r2_list.append(r2_score(y_train_pred, y_train)) \n",
    "        \n",
    "    y_test_pred_list=list(itertools.chain(*y_test_pred_list))\n",
    "    y_test_true_list=list(itertools.chain(*y_test_true_list))\n",
    "    test_rmse = mean_squared_error(y_test_true_list, y_test_pred_list,squared=False)\n",
    "    test_R2 = r2_score(y_test_true_list, y_test_pred_list)\n",
    "    train_rmse = np.mean(y_train_rmse_list)\n",
    "    train_R2 = np.mean(y_train_r2_list)\n",
    "    print('Train rmse: ',train_rmse)\n",
    "    print('Train R2: ',train_R2)\n",
    "    print('Test rmse: ',test_rmse)\n",
    "    print('Test R2: ',test_R2)\n",
    "    if fig:\n",
    "        figAccScatter(y_test_true_list,y_test_pred_list,unit=unit)\n",
    "    return [train_rmse,train_R2,test_rmse,test_R2]\n",
    "\n",
    "\n",
    "def ANN_TL_byTest(X_train,y_train,X_test,y_test,importbaseModelPath,unit,epochs=120,importbaseModel=None,learning_rate=0.00005,batch_size=32,fig=True):\n",
    "    '''\n",
    "    独立测试集验证迁移学习\n",
    "    '''\n",
    "    x_train = X_train\n",
    "    y_train = y_train\n",
    "    x_test = X_test\n",
    "    y_test = y_test\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    print(y_test)\n",
    "#迁移模型\n",
    "    if importbaseModel == None:\n",
    "        print(f'---------========== learning by import baseModel file ============-----------')\n",
    "        sklearn_model=None\n",
    "        sklearn_model = build_model(x_test_scaled,learning_rate=learning_rate)\n",
    "        sklearn_model.load_weights(importbaseModelPath)\n",
    "        newl_last_1 = Dense(1, activation=None)(sklearn_model.layers[-2].output)  \n",
    "        sklearn_model = Model(inputs=sklearn_model.input, outputs=[newl_last_1])\n",
    "    else:\n",
    "        print(f'---------========== learning by import baseModel variables(object) ===========------------')\n",
    "        sklearn_model=None\n",
    "        newl_last_1 = Dense(1, activation=None)(importbaseModel.layers[-2].output)  \n",
    "        sklearn_model = Model(inputs=importbaseModel.input, outputs=[newl_last_1])\n",
    "    \n",
    "    sklearn_model.summary()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    sklearn_model.compile(loss='mse', optimizer=optimizer,metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "#     callbacks = [keras.callbacks.EarlyStopping(patience=patience, min_delta=1e-2)]\n",
    "\n",
    "    \n",
    "    history = sklearn_model.fit(x_train_scaled, y_train,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test_scaled, y_test),\n",
    "#                             batch_size=batch_size,   \n",
    "                            verbose=2)\n",
    "    \n",
    "    \n",
    "    # 绘制学习曲线\n",
    "    print(history.params)\n",
    "    print_history(history) #调用绘图函数\n",
    "    \n",
    "    y_test_pred = sklearn_model.predict(x_test_scaled)\n",
    "    y_train_pred = sklearn_model.predict(x_train_scaled)\n",
    "\n",
    "    print(f'Train_rmse: {mean_squared_error(y_train_pred, y_train,squared=False)}')\n",
    "    print(f'Train_R2: {r2_score(y_train_pred, y_train)}') \n",
    "    print(f'Test_rmse: {mean_squared_error(y_test_pred, y_test,squared=False)}')\n",
    "    print(f'Test_R2: {r2_score(y_test_pred, y_test)}')\n",
    "    if fig:\n",
    "        figAccScatter(y_test,y_test_pred,unit=unit)\n",
    "\n",
    "    return sklearn_model\n",
    "\n",
    "#传统算法GBRT直接训练\n",
    "def GBRT_DL_byLOO(X,y,params,unit,fig=True):\n",
    "    '''\n",
    "    用于GBRT模型留一法训练\n",
    "    '''\n",
    "    loo = LeaveOneOut()\n",
    "    gbr = GBR(**params)\n",
    "    y_test_pred_list,y_test_true_list = [],[]\n",
    "    y_train_rmse_list,y_train_r2_list = [],[]\n",
    "    for train_i, test_i in loo.split(X):\n",
    "        x_train = X.iloc[train_i,:]\n",
    "        y_train = y[train_i]\n",
    "        x_test = X.iloc[test_i,:]\n",
    "        y_test = y[test_i]\n",
    "        gbr.fit(X.iloc[train_i,:], y[train_i])\n",
    "        y_test_pred = gbr.predict(X.iloc[test_i,:])\n",
    "        y_train_pred = gbr.predict(X.iloc[train_i,:])\n",
    "        y_test_pred_list.append(y_test_pred)\n",
    "        y_test_true_list.append(y[test_i])\n",
    "        \n",
    "        y_train_rmse_list.append(mean_squared_error(y_train_pred, y_train,squared=False))\n",
    "        y_train_r2_list.append(r2_score(y_train_pred, y_train)) \n",
    "    y_test_pred_list=list(itertools.chain(*y_test_pred_list))\n",
    "    y_test_true_list=list(itertools.chain(*y_test_true_list))\n",
    "    test_rmse = mean_squared_error(y_test_true_list, y_test_pred_list,squared=False)\n",
    "    test_R2 = r2_score(y_test_true_list, y_test_pred_list)\n",
    "    train_rmse = np.mean(y_train_rmse_list)\n",
    "    train_R2 = np.mean(y_train_r2_list)\n",
    "    print('Train rmse: ',train_rmse)\n",
    "    print('Train R2: ',train_R2)\n",
    "    print('Test rmse: ',test_rmse)\n",
    "    print('Test R2: ',test_R2)\n",
    "    if fig:\n",
    "        figAccScatter(y_test_true_list,y_test_pred_list,unit=unit)\n",
    "    return [train_rmse,train_R2,test_rmse,test_R2]\n",
    "\n",
    "def GBRT_DL_byCV(X,y,params,unit,fig=True,k=10):\n",
    "    '''\n",
    "    用于GBRT模型交叉验证训练\n",
    "    '''\n",
    "    kfold =  KFold(n_splits=k, shuffle=True\n",
    "                      , random_state=1234\n",
    "                     )\n",
    "    gbr = GBR(**params)\n",
    "    y_test_pred_list,y_test_true_list = [],[]\n",
    "    y_train_rmse_list,y_train_r2_list,y_test_rmse_list,y_test_r2_list = [],[],[],[]\n",
    "    for train_i, test_i in kfold.split(X):\n",
    "        gbr.fit(X.iloc[train_i,:], y[train_i])\n",
    "        y_test_pred = gbr.predict(X.iloc[test_i,:])\n",
    "        y_train_pred = gbr.predict(X.iloc[train_i,:])\n",
    "        y_test_pred_list.append(y_test_pred)\n",
    "        y_test_true_list.append(y[test_i])\n",
    "        \n",
    "        y_train_rmse_list.append(mean_squared_error(y_train_pred, y[train_i],squared=False))\n",
    "        y_train_r2_list.append(r2_score(y_train_pred, y[train_i])) \n",
    "    y_test_pred_list=list(itertools.chain(*y_test_pred_list))\n",
    "    y_test_true_list=list(itertools.chain(*y_test_true_list))\n",
    "    test_rmse = mean_squared_error(y_test_true_list, y_test_pred_list,squared=False)\n",
    "    test_R2 = r2_score(y_test_true_list, y_test_pred_list)\n",
    "    train_rmse = np.mean(y_train_rmse_list)\n",
    "    train_R2 = np.mean(y_train_r2_list)\n",
    "    print('Train rmse: ',train_rmse)\n",
    "    print('Train R2: ',train_R2)\n",
    "    print('Test rmse: ',test_rmse)\n",
    "    print('Test R2: ',test_R2)\n",
    "    if fig:\n",
    "        figAccScatter(y_test_true_list,y_test_pred_list,unit=unit)\n",
    "    return [train_rmse,train_R2,test_rmse,test_R2]\n",
    "\n",
    "\n",
    "def GBRT_DL_byTest(X_train,y_train,X_test,y_test,params,unit,fig=True):\n",
    "    '''\n",
    "    独立测试集验证GBRT直接学习\n",
    "    '''\n",
    "    \n",
    "    gbr = GBR(**params)\n",
    "    gbr.fit(X_train,y_train)\n",
    "    y_train_pred = gbr.predict(X_train)\n",
    "    y_test_pred = gbr.predict(X_test)\n",
    "    \n",
    "    test_rmse = mean_squared_error(y_test_pred, y_test,squared=False)\n",
    "    test_R2 = r2_score(y_test_pred, y_test)\n",
    "    train_rmse = mean_squared_error(y_train_pred, y_train,squared=False)\n",
    "    train_R2 = r2_score(y_train_pred, y_train)\n",
    "    print('Train rmse: ',train_rmse)\n",
    "    print('Train R2: ',train_R2)\n",
    "    print('Test rmse: ',test_rmse)\n",
    "    print('Test R2: ',test_R2)\n",
    "    if fig:\n",
    "        figAccScatter(y_test,y_test_pred,unit=unit)\n",
    "\n",
    "    return [train_rmse,train_R2,test_rmse,test_R2]\n",
    "\n",
    "\n",
    "\n",
    "def figAccScatter(y_test_true_list,y_test_pred_list,unit,s=70,color='#227C70'):\n",
    "    # 画散点图\n",
    "    theme = load_theme(\"boxy_light\")\n",
    "    theme.apply()\n",
    "    plt.rcParams['font.sans-serif']=['Times New Roman']\n",
    "    fig = plt.figure(figsize=[6,5],dpi=500)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    print(len(y_test_pred_list))\n",
    "    print('--------------')\n",
    "    print(len(y_test_true_list))\n",
    "    g = plt.scatter(y_test_true_list,y_test_pred_list,s=s,color=color)\n",
    "    plt.xticks(size=15)\n",
    "    plt.yticks(size=15)\n",
    "#     plt.xlim([np.min(y_test_true_list),np.max(y_test_true_list)])\n",
    "#     plt.ylim([np.min(y_test_pred_list),np.max(y_test_pred_list)])\n",
    "    \n",
    "    plt.xlabel(f'DFT value ({unit})',fontdict={'size':20})\n",
    "    plt.ylabel(f'ML predicted ({unit})',fontdict={'size':20})\n",
    "    x = np.arange(np.min(y_test_true_list),np.max(y_test_true_list),0.01)\n",
    "    y = x\n",
    "    plt.plot(x,y,'--',color='black',linewidth=1)\n",
    "    bwith=1\n",
    "    ax.spines['bottom'].set_linewidth(bwith)\n",
    "    ax.spines['left'].set_linewidth(bwith)\n",
    "    ax.spines['top'].set_linewidth(bwith)\n",
    "    ax.spines['right'].set_linewidth(bwith)\n",
    "    plt.show()\n",
    "    theme.apply_transforms()\n",
    "    \n",
    "#绘图函数\n",
    "def print_history(history):\n",
    "    # 绘制训练 & 验证的准确率值\n",
    "    fig = plt.figure(figsize=[6,5],dpi=300)\n",
    "    ax = plt.gca()\n",
    "    plt.plot(history.history['root_mean_squared_error'])\n",
    "    plt.plot(history.history['val_root_mean_squared_error'])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model rmse&loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train_rmse', 'Val_rmse', 'Train_loss', 'Val_loss'])\n",
    "    plt.show()\n",
    "\n",
    "def build_model(x_train,\n",
    "            hidden_layers=5,\n",
    "            layer_0_size=100,\n",
    "            layer_1_size=300,\n",
    "            layer_2_size=300,\n",
    "            layer_3_size=300,\n",
    "             layer_4_size=100,\n",
    "             layer_5_size=1024,\n",
    "             layer_6_size=1024,\n",
    "             layer_7_size=1024,\n",
    "             layer_8_size=1024,\n",
    "            learning_rate=0.00005,seed=123):\n",
    "    '''\n",
    "    ANN框架返回函数\n",
    "    '''\n",
    "    layer_size = [layer_0_size, layer_1_size, layer_2_size,\n",
    "                  layer_3_size,layer_4_size,layer_5_size,layer_6_size\n",
    "                 ,layer_7_size,layer_8_size]\n",
    "    tf.random.set_seed(seed)\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(layer_size[0], activation='relu',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "    for i in range(hidden_layers - 1):\n",
    "        model.add(keras.layers.Dense(layer_size[i+1],\n",
    "                                     activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "#     optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer,metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aecbb4-78ea-43e4-8a49-f9c7defb46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ehull\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from aquarel import load_theme\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "\n",
    "# 生成基模型\n",
    "def getBaseModelClassifier(X_train,y_train,X_test,y_test,unit,epochs=120,learning_rate=0.00005,batch_size=32,k=10,fig=True):\n",
    "    '''\n",
    "    训练返回基模型\n",
    "    '''\n",
    "    x_train = X_train\n",
    "    y_train = y_train\n",
    "    x_test = X_test\n",
    "    y_test = y_test\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    sklearn_model = build_modelClassifier(x_test_scaled,learning_rate=learning_rate)\n",
    "    sklearn_model.summary()\n",
    "    \n",
    "    history = sklearn_model.fit(x_train_scaled, y_train,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test_scaled, y_test),\n",
    "#                             batch_size=batch_size,   \n",
    "                            verbose=0)\n",
    "    # 绘制学习曲线\n",
    "    print(history.params)\n",
    "    print_historyClassifier(history) #调用绘图函数\n",
    "    \n",
    "    y_test_pred = sklearn_model.predict(x_test_scaled)\n",
    "    y_train_pred = sklearn_model.predict(x_train_scaled)\n",
    "\n",
    "    print(f'Train_rmse: {mean_squared_error(y_train_pred, y_train,squared=False)}')\n",
    "    print(f'Train_R2: {r2_score(y_train_pred, y_train)}') \n",
    "    print(f'Test_rmse: {mean_squared_error(y_test_pred, y_test,squared=False)}')\n",
    "    print(f'Test_R2: {r2_score(y_test_pred, y_test)}')\n",
    "    print(f'Train Dataset: {len(x_train_scaled)}')\n",
    "    if fig:\n",
    "        figAccScatter(y_test,y_test_pred,unit=unit)\n",
    "\n",
    "    return sklearn_model\n",
    "\n",
    "def ANNClassifier_DL_byCV(X,y,unit,k=10,patience=80,learning_rate=0.00005,fig=True):\n",
    "    '''\n",
    "    ANN 交叉验证直接训练\n",
    "    '''\n",
    "    #交叉验证训练\n",
    "    kfold =  KFold(n_splits=k, shuffle=True\n",
    "                      , random_state=1234\n",
    "                     )\n",
    "    \n",
    "    y_test_pred_list,y_test_true_list = [],[]\n",
    "    y_train_rmse_list,y_train_r2_list,y_test_rmse_list,y_test_r2_list = [],[],[],[]\n",
    "    count = 0\n",
    "    for train_i, test_i in kfold.split(X):\n",
    "        count+=1\n",
    "        print(f'-------================== LOOP {count}/{k} ===================-------')\n",
    "        x_train = X.iloc[train_i,:]\n",
    "        y_train = y[train_i]\n",
    "        x_test = X.iloc[test_i,:]\n",
    "        y_test = y[test_i]\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(x_train)\n",
    "        x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "        sklearn_model = build_model(x_test_scaled,learning_rate=learning_rate)\n",
    "        sklearn_model.summary()\n",
    "        callbacks = [keras.callbacks.EarlyStopping(patience=patience, min_delta=1e-2)]\n",
    "\n",
    "        history = sklearn_model.fit(x_train_scaled, y_train,\n",
    "                                epochs=1000,\n",
    "                                validation_data=(x_test_scaled, y_test),\n",
    "                                callbacks=callbacks,\n",
    "                                   verbose=0)\n",
    "                # 绘制学习曲线\n",
    "        print(history.params)\n",
    "        print_historyClassifier(history) #调用绘图函数\n",
    "    \n",
    "        y_test_pred = sklearn_model.predict(x_test_scaled)\n",
    "        y_train_pred = sklearn_model.predict(x_train_scaled)\n",
    "        y_test_pred_list.append(y_test_pred)\n",
    "        y_test_true_list.append(y_test)\n",
    "        \n",
    "        y_train_rmse_list.append(mean_squared_error(y_train_pred, y_train,squared=False))\n",
    "        y_train_r2_list.append(r2_score(y_train_pred, y_train)) \n",
    "    y_test_pred_list=list(itertools.chain(*y_test_pred_list))\n",
    "    y_test_true_list=list(itertools.chain(*y_test_true_list))\n",
    "    test_rmse = mean_squared_error(y_test_true_list, y_test_pred_list,squared=False)\n",
    "    test_R2 = r2_score(y_test_true_list, y_test_pred_list)\n",
    "    train_rmse = np.mean(y_train_rmse_list)\n",
    "    train_R2 = np.mean(y_train_r2_list)\n",
    "    print('Train rmse: ',train_rmse)\n",
    "    print('Train R2: ',train_R2)\n",
    "    print('Test rmse: ',test_rmse)\n",
    "    print('Test R2: ',test_R2)\n",
    "    if fig:\n",
    "        figAccScatter(y_test_true_list,y_test_pred_list,unit=unit)\n",
    "    return [train_rmse,train_R2,test_rmse,test_R2]\n",
    "\n",
    "\n",
    "\n",
    "def ANNClassifier_DL_byTest(X_train,y_train,X_test,y_test,unit,epochs=120,learning_rate=0.00005,batch_size=32,fig=True):\n",
    "    '''\n",
    "    独立测试集验证ANN直接学习\n",
    "    '''\n",
    "    x_train = X_train\n",
    "    y_train = y_train\n",
    "    x_test = X_test\n",
    "    y_test = y_test\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    sklearn_model = build_modelClassifier(x_test_scaled,learning_rate=learning_rate)\n",
    "    sklearn_model.summary()\n",
    "    callbacks = [keras.callbacks.EarlyStopping(patience=80, min_delta=1e-2)]\n",
    "\n",
    "    history = sklearn_model.fit(x_train_scaled, y_train,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test_scaled, y_test),\n",
    "#                             callbacks=callbacks,\n",
    "                               verbose=2)\n",
    "            # 绘制学习曲线\n",
    "    print(history.params)\n",
    "    print_historyClassifier(history) #调用绘图函数\n",
    "    \n",
    "    y_test_pred = sklearn_model.predict(x_test_scaled)\n",
    "    y_train_pred = sklearn_model.predict(x_train_scaled)\n",
    "    loss_test, accuracy_test = sklearn_model.evaluate(x_test_scaled, y_test)\n",
    "    loss_train, accuracy_train = sklearn_model.evaluate(x_train_scaled, y_train)\n",
    "    test_pred_res = sklearn_model.predict(x_test_scaled)\n",
    "    print(f'Train_accuracy: {accuracy_train}')\n",
    "    print(f'Test_accuracy: {accuracy_test}')\n",
    " \n",
    "    if fig:\n",
    "        figAccScatter(y_test,y_test_pred,unit=unit)\n",
    "\n",
    "    return sklearn_model,test_pred_res\n",
    "\n",
    "\n",
    "\n",
    "# 迁移学习后观察交叉验证结果\n",
    "def ANNClassifier_TL_byCV(X,y,importbaseModelPath,unit,patience=80,k=10,importbaseModel=None,learning_rate=0.00005,batch_size=32,fig=True):\n",
    "    '''\n",
    "    交叉验证迁移学习\n",
    "    '''\n",
    "    #交叉验证训练\n",
    "    kfold =  KFold(n_splits=k, shuffle=True\n",
    "                      , random_state=1234\n",
    "                     )\n",
    "    \n",
    "    y_test_pred_list,y_test_true_list = [],[]\n",
    "    y_train_rmse_list,y_train_r2_list,y_test_rmse_list,y_test_r2_list = [],[],[],[]\n",
    "    count = 0\n",
    "    for train_i, test_i in kfold.split(X):\n",
    "        count+=1\n",
    "        x_train = X.iloc[train_i,:]\n",
    "        y_train = y[train_i]\n",
    "        x_test = X.iloc[test_i,:]\n",
    "        y_test = y[test_i]\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(x_train)\n",
    "        x_test_scaled = scaler.transform(x_test)\n",
    "        #迁移模型\n",
    "        if importbaseModel == None:\n",
    "            print(f'---------========== learning by import baseModel file {count}/{k}============-----------')\n",
    "            sklearn_model=None\n",
    "            sklearn_model = build_model(x_test_scaled,learning_rate=learning_rate)\n",
    "            sklearn_model.load_weights(importbaseModelPath)\n",
    "            newl_last_1 = Dense(1, activation=None)(sklearn_model.layers[-2].output)  \n",
    "            sklearn_model = Model(inputs=sklearn_model.input, outputs=[newl_last_1])\n",
    "        else:\n",
    "            print(f'---------========== learning by import baseModel variables(object) {count}/{k}===========------------')\n",
    "            sklearn_model=None\n",
    "            newl_last_1 = Dense(1, activation=None)(importbaseModel.layers[-2].output)  \n",
    "            sklearn_model = Model(inputs=importbaseModel.input, outputs=[newl_last_1])\n",
    "        \n",
    "        sklearn_model.summary()\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        sklearn_model.compile(loss='mse', optimizer=optimizer,\n",
    "                              metrics=[keras.metrics.RootMeanSquaredError()]\n",
    "                             )\n",
    "        callbacks = [keras.callbacks.EarlyStopping(patience=patience, min_delta=1e-2)]\n",
    "\n",
    "        history = sklearn_model.fit(x_train_scaled, y_train,\n",
    "                                epochs=1000,\n",
    "#                                 batch_size=batch_size,\n",
    "                                validation_data=(x_test_scaled, y_test),\n",
    "                                callbacks=callbacks,\n",
    "                                   verbose=0)\n",
    "            # 绘制学习曲线\n",
    "        print(history.params)\n",
    "        print_historyClassifier(history) #调用绘图函数\n",
    "        \n",
    "        y_test_pred = sklearn_model.predict(x_test_scaled)\n",
    "        y_train_pred = sklearn_model.predict(x_train_scaled)\n",
    "        y_test_pred_list.append(y_test_pred)\n",
    "        y_test_true_list.append(y_test)\n",
    "        \n",
    "        y_train_rmse_list.append(mean_squared_error(y_train_pred, y_train,squared=False))\n",
    "        y_train_r2_list.append(r2_score(y_train_pred, y_train)) \n",
    "    y_test_pred_list=list(itertools.chain(*y_test_pred_list))\n",
    "    y_test_true_list=list(itertools.chain(*y_test_true_list))\n",
    "    test_rmse = mean_squared_error(y_test_true_list, y_test_pred_list,squared=False)\n",
    "    test_R2 = r2_score(y_test_true_list, y_test_pred_list)\n",
    "    train_rmse = np.mean(y_train_rmse_list)\n",
    "    train_R2 = np.mean(y_train_r2_list)\n",
    "    print('Train rmse: ',train_rmse)\n",
    "    print('Train R2: ',train_R2)\n",
    "    print('Test rmse: ',test_rmse)\n",
    "    print('Test R2: ',test_R2)\n",
    "    if fig:\n",
    "        figAccScatter(y_test_true_list,y_test_pred_list,unit=unit)\n",
    "    return [train_rmse,train_R2,test_rmse,test_R2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ANNClassifier_TL_byTest(X_train,y_train,X_test,y_test,importbaseModelPath,unit,epochs=120,importbaseModel=None,learning_rate=0.00005,batch_size=32,fig=True):\n",
    "    '''\n",
    "    独立测试集验证迁移学习\n",
    "    '''\n",
    "    x_train = X_train\n",
    "    y_train = y_train\n",
    "    x_test = X_test\n",
    "    y_test = y_test\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    print(y_test)\n",
    "#迁移模型\n",
    "    if importbaseModel == None:\n",
    "        print(f'---------========== learning by import baseModel file ============-----------')\n",
    "        sklearn_model=None\n",
    "        sklearn_model = build_model(x_test_scaled,learning_rate=learning_rate)\n",
    "        sklearn_model.load_weights(importbaseModelPath)\n",
    "        newl_last_1 = Dense(1, activation=None)(sklearn_model.layers[-2].output)  \n",
    "        sklearn_model = Model(inputs=sklearn_model.input, outputs=[newl_last_1])\n",
    "    else:\n",
    "        print(f'---------========== learning by import baseModel variables(object) ===========------------')\n",
    "        sklearn_model=None\n",
    "        newl_last_1 = Dense(1, activation='sigmoid')(importbaseModel.layers[-2].output)  \n",
    "        sklearn_model = Model(inputs=importbaseModel.input, outputs=[newl_last_1])\n",
    "    \n",
    "    sklearn_model.summary()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    sklearn_model.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "#     callbacks = [keras.callbacks.EarlyStopping(patience=patience, min_delta=1e-2)]\n",
    "\n",
    "    \n",
    "    history = sklearn_model.fit(x_train_scaled, y_train,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test_scaled, y_test),\n",
    "#                             batch_size=batch_size,   \n",
    "                            verbose=2)\n",
    "    \n",
    "    \n",
    "    print_historyClassifier(history) #调用绘图函数\n",
    "    \n",
    "    y_test_pred = sklearn_model.predict(x_test_scaled)\n",
    "    y_train_pred = sklearn_model.predict(x_train_scaled)\n",
    "    loss_test, accuracy_test = sklearn_model.evaluate(x_test_scaled, y_test)\n",
    "    loss_train, accuracy_train = sklearn_model.evaluate(x_train_scaled, y_train)\n",
    "    test_pred_res = sklearn_model.predict(x_test_scaled)\n",
    "    print(f'Train_accuracy: {accuracy_train}')\n",
    "    print(f'Test_accuracy: {accuracy_test}')\n",
    "    if fig:\n",
    "        figAccScatter(y_test,y_test_pred,unit=unit)\n",
    "\n",
    "    return sklearn_model,test_pred_res\n",
    "\n",
    "\n",
    "def GBRTClassifier_DL_byCV(X,y,params,unit,fig=True,k=10):\n",
    "    '''\n",
    "    用于GBRT模型交叉验证训练\n",
    "    '''\n",
    "    kfold =  KFold(n_splits=k, shuffle=True\n",
    "                      , random_state=1234\n",
    "                     )\n",
    "    gbr = GBR(**params)\n",
    "    y_test_pred_list,y_test_true_list = [],[]\n",
    "    y_train_rmse_list,y_train_r2_list,y_test_rmse_list,y_test_r2_list = [],[],[],[]\n",
    "    for train_i, test_i in kfold.split(X):\n",
    "        gbr.fit(X.iloc[train_i,:], y[train_i])\n",
    "        y_test_pred = gbr.predict(X.iloc[test_i,:])\n",
    "        y_train_pred = gbr.predict(X.iloc[train_i,:])\n",
    "        y_test_pred_list.append(y_test_pred)\n",
    "        y_test_true_list.append(y[test_i])\n",
    "        \n",
    "        y_train_rmse_list.append(mean_squared_error(y_train_pred, y[train_i],squared=False))\n",
    "        y_train_r2_list.append(r2_score(y_train_pred, y[train_i])) \n",
    "    y_test_pred_list=list(itertools.chain(*y_test_pred_list))\n",
    "    y_test_true_list=list(itertools.chain(*y_test_true_list))\n",
    "    test_rmse = mean_squared_error(y_test_true_list, y_test_pred_list,squared=False)\n",
    "    test_R2 = r2_score(y_test_true_list, y_test_pred_list)\n",
    "    train_rmse = np.mean(y_train_rmse_list)\n",
    "    train_R2 = np.mean(y_train_r2_list)\n",
    "    print('Train rmse: ',train_rmse)\n",
    "    print('Train R2: ',train_R2)\n",
    "    print('Test rmse: ',test_rmse)\n",
    "    print('Test R2: ',test_R2)\n",
    "    if fig:\n",
    "        figAccScatter(y_test_true_list,y_test_pred_list,unit=unit)\n",
    "    return [train_rmse,train_R2,test_rmse,test_R2]\n",
    "\n",
    "\n",
    "def GBRTClassifier_DL_byTest(X_train,y_train,X_test,y_test,params,unit,fig=True):\n",
    "    '''\n",
    "    独立测试集验证GBRT直接学习\n",
    "    '''\n",
    "    \n",
    "    gbc = GBC(**params)\n",
    "    gbc.fit(X_train,y_train)\n",
    "    y_train_pred = gbc.predict(X_train)\n",
    "    y_test_pred = gbc.predict(X_test)\n",
    "    \n",
    "    accuracy_test = gbc.score(X_test, y_test)\n",
    "    accuracy_train = gbc.score(X_train,y_train)\n",
    "    print(f'Train_accuracy: {accuracy_train}')\n",
    "    print(f'Test_accuracy: {accuracy_test}')\n",
    "    if fig:\n",
    "        figAccScatter(y_test,y_test_pred,unit=unit)\n",
    "\n",
    "    return gbc\n",
    "\n",
    "\n",
    "\n",
    "def build_modelClassifier(x_train,\n",
    "            hidden_layers=5,\n",
    "            layer_0_size=100,\n",
    "            layer_1_size=300,\n",
    "            layer_2_size=300,\n",
    "            layer_3_size=300,\n",
    "             layer_4_size=100,\n",
    "             layer_5_size=1024,\n",
    "             layer_6_size=1024,\n",
    "             layer_7_size=1024,\n",
    "             layer_8_size=1024,\n",
    "            learning_rate=0.00005,seed=123):\n",
    "    '''\n",
    "    ANN框架返回函数\n",
    "    '''\n",
    "    layer_size = [layer_0_size, layer_1_size, layer_2_size,\n",
    "                  layer_3_size,layer_4_size,layer_5_size,layer_6_size\n",
    "                 ,layer_7_size,layer_8_size]\n",
    "    tf.random.set_seed(seed)\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(layer_size[0], activation='relu',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "    for i in range(hidden_layers - 1):\n",
    "        model.add(keras.layers.Dense(layer_size[i+1],\n",
    "                                     activation='relu'))\n",
    "    model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "#     optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#绘图函数\n",
    "def print_historyClassifier(history):\n",
    "    # 绘制训练 & 验证的准确率值\n",
    "    fig = plt.figure(figsize=[6,5],dpi=300)\n",
    "    ax = plt.gca()\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model accuracy&loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train_accuracy', 'Val_accuracy', 'Train_loss', 'Val_loss'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe51abe-9a5b-4b38-85aa-2b41dd5ea43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseModel_B1_1 = getBaseModel(pd.concat([X_D1_train,X_D1_test],axis=0).reset_index(drop=True) ,pd.concat([y_Ef_D1_train,y_Ef_D1_test],axis=0).reset_index(drop=True) ,X_D1_test,y_Ef_D1_test,\n",
    "                              unit='eV/atom',epochs=50,\n",
    "                             learning_rate=0.00005\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49467a1-d1b9-4be5-b58f-01c976824e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_B1 = keras.models.load_model('保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.0251_TLbest_20221231.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f552bb-3134-4a0a-8e40-bf4cb3ce2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bandgap\n",
    "## GBRT交叉验证直接训练\n",
    "GBRT_DL_byCV(X_D1_train,\n",
    "             y_Bg_D1_train,\n",
    "             params = {'n_estimators': 200,'subsample': 0.9, 'max_depth':5,\n",
    "            'max_features':5,'random_state':1},\n",
    "             unit='eV',fig=True,k=5)\n",
    "GBRT_DL_byTest(X_D1_train,y_Bg_D1_train,X_D1_test,y_Bg_D1_test,\n",
    "             params = {'n_estimators': 200,'subsample': 0.9, 'max_depth':5,\n",
    "            'max_features':5,'random_state':1},\n",
    "             unit='eV',fig=True)\n",
    "## ANN 交叉验证直接训练\n",
    "ANN_DL_byCV(X_D1_train,y_Bg_D1_train,unit='eV',learning_rate=0.0005,k=10,patience=80,fig=True)\n",
    "## ANN 独立测试集直接训练\n",
    "ANN_DL_byTest(X_D1_train,y_Bg_D1_train,X_D1_test,y_Bg_D1_test,unit='eV',epochs=120,learning_rate=0.0005,batch_size=32,fig=True)\n",
    "#Transfer Learning\n",
    "ANN_TL_byCV(X_D1_train,y_Bg_D1_train,importbaseModelPath='',unit='eV',\n",
    "            importbaseModel = base_model_B1,\n",
    "            learning_rate=0.0003,)\n",
    "model_Bg_D1_B1_1_TL = ANN_TL_byTest(X_D1_train,y_Bg_D1_train,X_D1_test,y_Bg_D1_test,importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.0251_TLbest_20221231.h5',\n",
    "              unit='eV',epochs=100,\n",
    "            importbaseModel=base_model_B1,\n",
    "            learning_rate=0.0005,)\n",
    "model_Bg_D1_B1_1_TL = ANN_TL_byTest(X_D1_train,y_Bg_D1_train,X_D1_test,y_Bg_D1_test,importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.0251_TLbest_20221231.h5',\n",
    "              unit='eV',epochs=100,\n",
    "            importbaseModel=base_model_B1,\n",
    "            learning_rate=0.0005,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1494ba-79fa-49f3-ace0-1ea433ae28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ehull\n",
    "## GBRT交叉验证直接训练\n",
    "GBRT_DL_byCV(X_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "             y_ehull_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "             params = {'n_estimators': 200,'subsample': 0.9, 'max_depth':5,\n",
    "            'max_features':5,'random_state':1},\n",
    "             unit='eV/atom',fig=True,k=10)\n",
    "GBRT_DL_byTest(X_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "               y_ehull_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "               X_D1_test[y_ehull_D1_test<2.5].reset_index(drop=True),\n",
    "               y_ehull_D1_test[y_ehull_D1_test<2.5].reset_index(drop=True),\n",
    "                params = {'n_estimators': 200,'subsample': 0.9, 'max_depth':5,\n",
    "               'max_features':5,'random_state':1},\n",
    "                unit='eV/atom',fig=True)\n",
    "## ANN 交叉验证直接训练\n",
    "ANN_DL_byCV(X_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "            y_ehull_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "            unit='eV/atom',learning_rate=0.0005,k=10,patience=80,fig=True)\n",
    "## ANN 独立测试集直接训练\n",
    "ANN_DL_byTest(X_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "            y_ehull_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "            X_D1_test[y_ehull_D1_test<2.5].reset_index(drop=True),\n",
    "            y_ehull_D1_test[y_ehull_D1_test<2.5].reset_index(drop=True),unit='eV/atom',epochs=120,\n",
    "            learning_rate=0.0005,batch_size=32,fig=True)\n",
    "## ANN 独立测试集直接训练\n",
    "ANN_DL_byTest(X_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "            y_ehull_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "            X_D1_test[y_ehull_D1_test<2.5].reset_index(drop=True),\n",
    "            y_ehull_D1_test[y_ehull_D1_test<2.5].reset_index(drop=True),unit='eV/atom',epochs=120,\n",
    "            learning_rate=0.0005,batch_size=32,fig=True)\n",
    "#TL\n",
    "ANN_TL_byCV(X_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "            y_ehull_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "            importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.019_20221228.h5',unit='eV/atom',\n",
    "            importbaseModel=base_model_B1,\n",
    "            learning_rate=0.00005,)\n",
    "model_ehull_D1_B1_1_TL = ANN_TL_byTest(X_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "                                       y_ehull_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "                                       X_D1_test[y_ehull_D1_test<2.5].reset_index(drop=True),\n",
    "                                       y_ehull_D1_test[y_ehull_D1_test<2.5].reset_index(drop=True),\n",
    "                                       importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.019_20221228.h5',\n",
    "                                      unit='eV/atom',epochs=120,\n",
    "                                    importbaseModel=base_model_B1,\n",
    "                                    learning_rate=0.005,)\n",
    "model_ehull_D1_B1_1_TL_2 = ANN_TL_byTest(X_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "                                       y_ehull_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "                                       X_D1_test[y_ehull_D1_test<2.5].reset_index(drop=True),\n",
    "                                       y_ehull_D1_test[y_ehull_D1_test<2.5].reset_index(drop=True),\n",
    "                                       importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.019_20221228.h5',\n",
    "                                      unit='eV/atom',epochs=120,\n",
    "                                    importbaseModel=base_model_B1,\n",
    "                                    learning_rate=0.0005,)\n",
    "model_ehull_D1_B1_1_TL_3 = ANN_TL_byTest(X_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "                                       y_ehull_D1_train[y_ehull_D1_train<2.5].reset_index(drop=True),\n",
    "                                       X_D1_test[y_ehull_D1_test<2.5].reset_index(drop=True),\n",
    "                                       y_ehull_D1_test[y_ehull_D1_test<2.5].reset_index(drop=True),\n",
    "                                       importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.019_20221228.h5',\n",
    "                                      unit='eV/atom',epochs=120,\n",
    "                                    importbaseModel=base_model_B1,\n",
    "                                    learning_rate=0.005,)\n",
    "#classification\n",
    "base_modelClassifier_B1,test_pred_res_B1=ANNClassifier_DL_byTest(X_D1_train,\n",
    "                        pd.cut(y_Ef_D1_train, bins=[-5, -2.0,10], labels=[1,0]),\n",
    "                        X_D1_test,\n",
    "                        pd.cut(y_Ef_D1_test, bins=[-5, -2.0,10], labels=[1,0]),\n",
    "                        unit='eV/atom',epochs=120,\n",
    "                        learning_rate=0.0005,batch_size=32,fig=False)\n",
    "model_ehull_D1_GBRT_DL_1 = GBRTClassifier_DL_byTest(X_D1_train,\n",
    "                df_D1_train['e_above_hull_class'],\n",
    "                X_D1_test,\n",
    "                df_D1_test['e_above_hull_class'],\n",
    "                 params = {'n_estimators': 100,'subsample': 0.9, 'max_depth':5,\n",
    "                   'max_features':4,'random_state':1},unit='eV/atom',fig=False)\n",
    "model_ehull_D1_GBRT_DL_1 = GBRTClassifier_DL_byTest(X_D1_train,\n",
    "                df_D1_train['e_above_hull_class'],\n",
    "                X_D1_test,\n",
    "                df_D1_test['e_above_hull_class'],\n",
    "                 params = {'n_estimators': 100,'subsample': 0.9, 'max_depth':5,\n",
    "                   'max_features':4,'random_state':1},unit='eV/atom',fig=False)\n",
    "def classif_Fig(y, yp):\n",
    "    '''\n",
    "    y: 真实值\n",
    "    y_pred:预测值\n",
    "    '''\n",
    "    '''混淆矩阵绘画'''\n",
    "    from sklearn.metrics import confusion_matrix  # 导入混淆矩阵函数\n",
    "    import matplotlib.pyplot as plt  # 导入作图库\n",
    "    plt.rcParams['font.sans-serif']=['Times New Roman']\n",
    "    fig = plt.figure(dpi=300) # 设置图片分辨率为 300 dpi\n",
    "    \n",
    "    cm = confusion_matrix(y, yp)  # 混淆矩阵\n",
    "    \n",
    "    print('混淆矩阵为：\\n',cm)\n",
    "    \n",
    "    plt.matshow(cm, cmap=plt.cm.Blues)  # 画混淆矩阵图，配色风格使用cm.Greens，更多风格请参考官网。\n",
    "    plt.colorbar()  # 颜色标签\n",
    " \n",
    "    for x in range(len(cm)):  # 数据标签\n",
    "        for y in range(len(cm)):\n",
    "            plt.annotate(cm[x, y], xy=(y, x),verticalalignment='center',horizontalalignment='center')\n",
    "            # 这边一个重要的易错点就是关于xy需要添加数值的位置的点要换成（y, x），因为矩阵可视化的xy与实际意义上的坐标是相反的\n",
    "    plt.ylabel('True label')  # 坐标轴标签\n",
    "    plt.xlabel('Predicted label')  # 坐标轴标签\n",
    "    plt.show\n",
    "classif_Fig(df_D1_test['e_above_hull_class'], model_ehull_D1_GBRT_DL_1.predict(X_D1_test))\n",
    "def classHot(test_pred_res):\n",
    "    lis = []\n",
    "    for i in test_pred_res:\n",
    "        if i[0]>=0.5:\n",
    "            lis.append(1)\n",
    "        else:\n",
    "            lis.append(0)\n",
    "    return lis\n",
    "\n",
    "classif_Fig(df_D1_test['e_above_hull_class'], classHot(test_pred_res))\n",
    "classif_Fig(pd.cut(y_Ef_D1_test, bins=[-5, -2.0,10], labels=[1,0]), classHot(test_pred_res_B1))\n",
    "classif_Fig(df_D1_test['e_above_hull_class'], classHot(test_pred_res_TL_1))\n",
    "model_ehull_D1_ANN_DL_1,test_pred_res = ANNClassifier_DL_byTest(X_D1_train,\n",
    "            df_D1_train['e_above_hull_class'],\n",
    "            X_D1_test,\n",
    "            df_D1_test['e_above_hull_class'],\n",
    "            unit='eV/atom',epochs=100,\n",
    "            learning_rate=0.005,batch_size=32,fig=False)\n",
    "model_ehull_D1_ANN_TL_1,test_pred_res_TL_1=ANNClassifier_TL_byTest(X_D1_train,\n",
    "            df_D1_train['e_above_hull_class'],\n",
    "            X_D1_test,\n",
    "            df_D1_test['e_above_hull_class'],\n",
    "            importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.019_20221228.h5',\n",
    "            importbaseModel=base_modelClassifier_B1,\n",
    "            unit='eV/atom',epochs=120,\n",
    "            learning_rate=0.005,batch_size=32,fig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1cde40-db08-45b9-a02a-8f6fc8f03026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buik modulus\n",
    "## GBRT交叉验证直接训练\n",
    "GBRT_DL_byCV(X_D1_B,y_B_D1,\n",
    "             params = {'n_estimators': 30,'subsample': 0.9, 'max_depth':4,\n",
    "            'max_features':4,'random_state':1},\n",
    "             unit='GPa',fig=True,k=10)\n",
    "## GBRT 留一法直接训练\n",
    "GBRT_DL_byLOO(X_D1_B,y_B_D1,\n",
    "             params = {'n_estimators': 30,'subsample': 0.9, 'max_depth':4,\n",
    "            'max_features':4,'random_state':1},\n",
    "             unit='GPa',fig=True)\n",
    "## GBRT 独立测试集直接训练 \n",
    "GBRT_DL_byTest(X_D1_B_train,y_B_D1_train,X_D1_B_test,y_B_D1_test,\n",
    "               params = {'n_estimators': 30,'subsample': 0.9, 'max_depth':4,\n",
    "                'max_features':4,'random_state':1},\n",
    "               unit='GPa',fig=True)\n",
    "## ANN 交叉验证直接训练\n",
    "ANN_DL_byCV(X_D1_B,y_B_D1,unit='GPa',k=10,patience=80,fig=True)\n",
    "## ANN 留一法直接训练\n",
    "ANN_DL_byLOO(X_D1_B,y_B_D1,unit='GPa',patience=80,fig=True)\n",
    "## ANN 独立测试集直接训练\n",
    "ANN_DL_byTest(X_D1_G_train,y_G_D1_train,X_D1_G_test,y_G_D1_test,unit='GPa',epochs=120,learning_rate=0.00005,batch_size=32,fig=True)\n",
    "## ANN 独立测试集直接训练20230108\n",
    "ANN_DL_byTest(X_D1_G_train,y_G_D1_train,X_D1_G_test,y_G_D1_test,unit='GPa',\n",
    "              epochs=120,learning_rate=0.0001,batch_size=32,fig=True)\n",
    "##TL\n",
    "## ANN 交叉验证迁移学习\n",
    "ANN_TL_byCV(X_D1_B,y_B_D1,\n",
    "            importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.0251_TLbest_20221231.h5',\n",
    "            unit='GPa',\n",
    "            learning_rate=0.00001,)\n",
    "## ANN 留一法迁移学习\n",
    "ANN_TL_byLOO(X_D1_B,y_B_D1,\n",
    "            importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.0251_TLbest_20221231.h5',\n",
    "            unit='GPa',\n",
    "            learning_rate=0.00001,)\n",
    "## ANN 独立测试集迁移学习123 51 20230108 \n",
    "model_B_D1_B1_1_TL_5 = ANN_TL_byTest(X_D1_B_train,y_B_D1_train,X_D1_B_test,y_B_D1_test,importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.019_20221228.h5',\n",
    "              unit='GPa',epochs=120,\n",
    "#              importbaseModel=base_model_B1,\n",
    "            learning_rate=0.00005,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2231c4-2944-4d45-937c-8397f382e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shear modulus\n",
    "## GBRT交叉验证直接训练\n",
    "GBRT_DL_byCV(X_D1_G,y_G_D1,\n",
    "             params = {'n_estimators': 30,'subsample': 0.9, 'max_depth':4,\n",
    "            'max_features':4,'random_state':1},\n",
    "             unit='GPa',fig=True,k=10)\n",
    "## GBRT 留一法直接训练\n",
    "GBRT_DL_byLOO(X_D1_G,y_G_D1,\n",
    "             params = {'n_estimators': 30,'subsample': 0.9, 'max_depth':4,\n",
    "            'max_features':4,'random_state':1},\n",
    "             unit='GPa',fig=True)\n",
    "## GBRT 独立测试集直接训练\n",
    "GBRT_DL_byTest(X_D1_G_train,y_G_D1_train,X_D1_G_test,y_G_D1_test,\n",
    "               params = {'n_estimators': 30,'subsample': 0.9, 'max_depth':4,\n",
    "                'max_features':4,'random_state':1},\n",
    "               unit='GPa',fig=True)\n",
    "## ANN 交叉验证直接训练\n",
    "ANN_DL_byCV(X_D1_G,y_G_D1,unit='GPa',k=10,patience=80,fig=True)\n",
    "## ANN 留一法直接训练\n",
    "ANN_DL_byLOO(X_D1_G,y_G_D1,unit='GPa',patience=80,fig=True)\n",
    "## ANN 独立测试集直接训练\n",
    "ANN_DL_byTest(X_D1_G_train,y_G_D1_train,X_D1_G_test,y_G_D1_test,unit='GPa',epochs=120,learning_rate=0.00005,batch_size=32,fig=True)\n",
    "import os\n",
    "def set_seeds(seed=12):\n",
    "    random_seed = 12 \n",
    "    tf.random.set_seed(random_seed )  # set random seed for tensorflow-cpu\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1' # set random seed for tensorflow-gpu\n",
    "\n",
    "set_seeds()\n",
    "ANN_TL_byCV(X_D1_G,y_G_D1,\n",
    "            importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.0251_TLbest_20221231.h5',\n",
    "            unit='GPa',\n",
    "            importbaseModel=base_model_B1,\n",
    "            learning_rate=0.00005,)\n",
    "ANN_TL_byLOO(X_D1_G,y_G_D1,\n",
    "            importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.0251_TLbest_20221231.h5',\n",
    "            unit='GPa',\n",
    "            importbaseModel=base_model_B1,\n",
    "            learning_rate=0.00005,)\n",
    "##continuous Transfer\n",
    "model_G_D1_B1_1_TL = ANN_TL_byTest(X_D1_G_train,y_G_D1_train,X_D1_G_test,y_G_D1_test,importbaseModelPath='保存模型/model_B_D1_B1_1_TL.h5',\n",
    "              unit='GPa',epochs=30,\n",
    "#             importbaseModel=model_B_D1_B1_1_TL_3,\n",
    "            learning_rate=0.0005,)\n",
    "model_G_D1_B1_1_TL.save('保存模型/model_G_D1_B1_1_TL_20230310.h5')\n",
    "model_G_D1_B1_1_TL_2 = ANN_TL_byTest(X_D1_G_train,y_G_D1_train,X_D1_G_test,y_G_D1_test,importbaseModelPath='保存模型/model_B_D1_B1_1_TL.h5',\n",
    "              unit='GPa',epochs=30,\n",
    "#             importbaseModel=model_B_D1_B1_1_TL_3,\n",
    "      model_G_D1_B1_1_TL = ANN_TL_byTest(pd.concat([X_D1_G_train,X_D5_G_train]).reset_index(drop=True),\n",
    "                                   pd.concat([y_G_D1_train,y_G_D5_train]).reset_index(drop=True),\n",
    "                                   X_D1_G_test,\n",
    "                                   y_G_D1_test,\n",
    "                                   importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.025_20221228.h5',\n",
    "                                  unit='GPa',epochs=100,\n",
    "                                importbaseModel=base_model_B1,\n",
    "                                learning_rate=0.00005,)      learning_rate=0.0005,)\n",
    "model_G_D1_B1_1_TL_5 = ANN_TL_byTest(X_D1_G_train,y_G_D1_train,X_D1_G_test,y_G_D1_test,importbaseModelPath='保存模型/halide_double_perovskite_Ef_132magpei_5layers_0.025_20221228.h5',\n",
    "              unit='GPa',epochs=120,\n",
    "            importbaseModel=model_B_D1_B1_1_TL_6,\n",
    "            learning_rate=0.00005,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e62fa-5b53-4a90-8a40-c4e1f8943327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#screening\n",
    "df_predictSet = pd.read_excel('表数据/df_predictSet.xlsx')\n",
    "df_predictSet = df_predictSet.drop(columns='Unnamed: 0')\n",
    "df_predictSet\n",
    "df_D1_predict_t = screen_by_ToleranceFactor(df_predictSet.reset_index(drop=True))\n",
    "df_D1_predict_t\n",
    "def drop_TrainData_from_predictData(df,df_1):  \n",
    "    import re\n",
    "    new_formula_list = []\n",
    "    for formu in df['pretty_formula']:\n",
    "        div = re.compile(r\"([A-Z]{1}[a-z]*)(\\d*)\")\n",
    "        ele_list = div.findall(formu)\n",
    "        if len(ele_list)==4:\n",
    "            new_formula = ele_list[0][0]+ele_list[0][1]+ele_list[2][0]+\\\n",
    "            ele_list[2][1]+ele_list[1][0]+ele_list[1][1]+ele_list[3][0]+ele_list[3][1]\n",
    "            new_formula_list.append(new_formula)\n",
    "    df_new_formula = pd.DataFrame(new_formula_list,columns=['pretty_formula'])\n",
    "\n",
    "    total_formula = pd.concat([pd.DataFrame(df['pretty_formula']),df_new_formula],axis=0)\n",
    "    index_list = []\n",
    "    for index, formula in df_1['formula'].items():\n",
    "        if formula not in list(total_formula['pretty_formula']):\n",
    "            index_list.append(index)\n",
    "    return df_1.loc[index_list,:]\n",
    "df_D1_predict_t_del=drop_TrainData_from_predictData(df_D1,df_D1_predict_t)\n",
    "df_D1_predict_t_del\n",
    "def predict_B_by_ANN(x_train,x_test,ANN_model):\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    y_predict = ANN_model.predict(x_test_scaled)\n",
    "    return pd.DataFrame(y_predict,columns=['B_predict(GPa)'])\n",
    "predict_B_by_ANN(X_D1_B_train,df_predictSet[df_predictSet['formula']=='Cs2LiVCl6'].loc[:,'MagpieData minimum Number':'MagpieData mode SpaceGroupNumber'],\n",
    "                 model_B_D1_B1_1_TL_2 )\n",
    "def predict_G_by_ANN(x_train,x_test,ANN_model):\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    y_predict = ANN_model.predict(x_test_scaled)\n",
    "    return pd.DataFrame(y_predict,columns=['G_predict(GPa)'])\n",
    "predict_G_by_ANN(X_D1_G_train,df_predictSet[df_predictSet['formula']=='Cs2LiVCl6'].loc[:,'MagpieData minimum Number':'MagpieData mode SpaceGroupNumber'],\n",
    "                 model_G_D1_B1_1_TL)\n",
    "def predict_B_by_ANN(x_train,x_test,ANN_model):\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    y_predict = ANN_model.predict(x_test_scaled)\n",
    "    return pd.DataFrame(y_predict,columns=['B_predict(GPa)'])\n",
    "df_D1_predict_B=pd.concat([ df_predictSet,predict_B_by_ANN(X_D1_G_train,df_predictSet.loc[:,'MagpieData minimum Number':'MagpieData mode SpaceGroupNumber'],\n",
    "                 keras.models.load_model('保存模型/model_B_D1_B1_1_TL_diff=7_R2=0.94_20230310.h5') )],axis=1)\n",
    "df_D1_predict_B\n",
    "def predict_G_by_ANN(x_train,x_test,ANN_model):\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    y_predict = ANN_model.predict(x_test_scaled)\n",
    "    return pd.DataFrame(y_predict,columns=['G_predict(GPa)'])\n",
    "df_D1_predict_B_G=pd.concat([ df_D1_predict_B,predict_G_by_ANN(X_D1_G_train,df_D1_predict_B.loc[:,'MagpieData minimum Number':'MagpieData mode SpaceGroupNumber'],\n",
    "                 keras.models.load_model('保存模型/model_G_D1_B1_1_TL_20230310.h5') )],axis=1)\n",
    "df_D1_predict_B_G\n",
    "df_D1_predict_B_G.to_excel('表数据/18040体模量剪切模量预测值20230311.xlsx',index=False)\n",
    "def drop_TrainData_from_predictData(df,df_1):  \n",
    "    import re\n",
    "    new_formula_list = []\n",
    "    for formu in df['pretty_formula']:\n",
    "        div = re.compile(r\"([A-Z]{1}[a-z]*)(\\d*)\")\n",
    "        ele_list = div.findall(formu)\n",
    "        if len(ele_list)==4:\n",
    "            new_formula = ele_list[0][0]+ele_list[0][1]+ele_list[2][0]+\\\n",
    "            ele_list[2][1]+ele_list[1][0]+ele_list[1][1]+ele_list[3][0]+ele_list[3][1]\n",
    "            new_formula_list.append(new_formula)\n",
    "    df_new_formula = pd.DataFrame(new_formula_list,columns=['pretty_formula'])\n",
    "\n",
    "    total_formula = pd.concat([pd.DataFrame(df['pretty_formula']),df_new_formula],axis=0)\n",
    "    index_list = []\n",
    "    for index, formula in df_1['formula'].items():\n",
    "        if formula not in list(total_formula['pretty_formula']):\n",
    "            index_list.append(index)\n",
    "    return df_1.loc[index_list,:]\n",
    "df_D1_predict_B_G_del=drop_TrainData_from_predictData(df_D1,df_D1_predict_B_G)\n",
    "df_D1_predict_B_G_del\n",
    "df_D1_predict_B_G_t_del = screen_by_ToleranceFactor(df_D1_predict_B_G_del.reset_index(drop=True))\n",
    "df_D1_predict_B_G_t_del\n",
    "df_D1_predict_B_G_t_del.to_excel('表数据/16529体模量剪切模量预测值新t20230313.xlsx',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae6f14-7e0d-491a-b492-07fae5712002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ehull\n",
    "## 分类预测\n",
    "def predict_ehull_by_GBRT(x_test,gbr_model):\n",
    "    y_predict = gbr_model.predict(x_test)\n",
    "    return pd.DataFrame(y_predict,columns=['Ehull_predict(eV/atom)'])\n",
    "df_D1_predict_ehull = pd.concat([df_predictSet,predict_ehull_by_GBRT(df_predictSet.loc[:,'MagpieData minimum Number':'MagpieData mode SpaceGroupNumber'],\n",
    "                     model_ehull_D1_GBRT_DL_1,)],axis=1)\n",
    "df_D1_predict_ehull\n",
    "def predict_Bg_by_ANN(x_train,x_test,ANN_model):\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    y_predict = ANN_model.predict(x_test_scaled)\n",
    "    return pd.DataFrame(y_predict,columns=['Bg_predict(eV)'])\n",
    "df_D1_predict_ehull_Eg = pd.concat([df_D1_predict_ehull,predict_Bg_by_ANN(X_D1_train,\n",
    "                        df_predictSet.loc[:,'MagpieData minimum Number':'MagpieData mode SpaceGroupNumber'],\n",
    "                         model_Bg_D1_B1_1_TL,\n",
    "                    )],axis=1)\n",
    "df_D1_predict_ehull_Eg\n",
    "df_D1_predict_ehull_Eg_t = screen_by_ToleranceFactor(df_D1_predict_ehull_Eg.reset_index(drop=True))\n",
    "df_D1_predict_ehull_Eg_t\n",
    "def predict_B_by_ANN(x_train,x_test,ANN_model):\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    y_predict = ANN_model.predict(x_test_scaled)\n",
    "    return pd.DataFrame(y_predict,columns=['B_predict(GPa)'])\n",
    "df_D1_predict_ehull_Eg_t_B=pd.concat([ df_D1_predict_ehull_Eg_t,predict_B_by_ANN(X_D1_B_train,df_D1_predict_ehull_Eg_t.loc[:,'MagpieData minimum Number':'MagpieData mode SpaceGroupNumber'],\n",
    "                 model_B_D1_B1_1_TL_6 )],axis=1)\n",
    "df_D1_predict_ehull_Eg_t_B\n",
    "def predict_G_by_ANN(x_train,x_test,ANN_model):\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    y_predict = ANN_model.predict(x_test_scaled)\n",
    "    return pd.DataFrame(y_predict,columns=['G_predict(GPa)'])\n",
    "df_D1_predict_ehull_Eg_t_B_G=pd.concat([ df_D1_predict_ehull_Eg_t_B,predict_G_by_ANN(X_D1_G_train,df_D1_predict_ehull_Eg_t_B.loc[:,'MagpieData minimum Number':'MagpieData mode SpaceGroupNumber'],\n",
    "                 model_G_D1_B1_1_TL_4 )],axis=1)\n",
    "df_D1_predict_ehull_Eg_t_B_G\n",
    "def drop_TrainData_from_predictData(df,df_1):  \n",
    "    import re\n",
    "    new_formula_list = []\n",
    "    for formu in df['pretty_formula']:\n",
    "        div = re.compile(r\"([A-Z]{1}[a-z]*)(\\d*)\")\n",
    "        ele_list = div.findall(formu)\n",
    "        if len(ele_list)==4:\n",
    "            new_formula = ele_list[0][0]+ele_list[0][1]+ele_list[2][0]+\\\n",
    "            ele_list[2][1]+ele_list[1][0]+ele_list[1][1]+ele_list[3][0]+ele_list[3][1]\n",
    "            new_formula_list.append(new_formula)\n",
    "    df_new_formula = pd.DataFrame(new_formula_list,columns=['pretty_formula'])\n",
    "\n",
    "    total_formula = pd.concat([pd.DataFrame(df['pretty_formula']),df_new_formula],axis=0)\n",
    "    index_list = []\n",
    "    for index, formula in df_1['formula'].items():\n",
    "        if formula not in list(total_formula['pretty_formula']):\n",
    "            index_list.append(index)\n",
    "    return df_1.loc[index_list,:]\n",
    "df_D1_predict_ehull_Eg_t_B_G_del=drop_TrainData_from_predictData(df_D1,df_D1_predict_ehull_Eg_t_B_G)\n",
    "df_D1_predict_ehull_Eg_t_B_G_del\n",
    "df_D1_predict_ehull_Eg_t_B_G_del[(df_D1_predict_ehull_Eg_t_B_G_del['Ehull_predict(eV/atom)']==1) & \n",
    "                         (df_D1_predict_ehull_Eg_t_B_G_del['Bg_predict(eV)']*1.21+0.36>=1) &\n",
    "                         (df_D1_predict_ehull_Eg_t_B_G_del['Bg_predict(eV)']*1.21+0.36<=1.7) &\n",
    "                        (df_D1_predict_ehull_Eg_t_B_G_del['new_t']<4.18) &\n",
    "                         (df_D1_predict_ehull_Eg_t_B_G_del['new_t']>0) \n",
    "                         & (df_D1_predict_ehull_Eg_t_B_G_del['G_predict(GPa)']/df_D1_predict_ehull_Eg_t_B_G_del['B_predict(GPa)']<0.2)\n",
    "                         \n",
    "                        ]\n",
    "df_D1_predict_ehull_Eg_t_B_G_del[(df_D1_predict_ehull_Eg_t_B_G_del['Ehull_predict(eV/atom)']==1) & \n",
    "                         (df_D1_predict_ehull_Eg_t_B_G_del['Bg_predict(eV)']*1.21+0.36>=1) &\n",
    "                         (df_D1_predict_ehull_Eg_t_B_G_del['Bg_predict(eV)']*1.21+0.36<=1.7) &\n",
    "                        (df_D1_predict_ehull_Eg_t_B_G_del['new_t']<4.18) &\n",
    "                         (df_D1_predict_ehull_Eg_t_B_G_del['new_t']>0) \n",
    "                         & ((df_D1_predict_ehull_Eg_t_B_G_del['G_predict(GPa)']/df_D1_predict_ehull_Eg_t_B_G_del['B_predict(GPa)'])<0.3)\n",
    "                         \n",
    "                        ]\n",
    "df_D1_predict_ehull_Eg_t_B_G_del[(df_D1_predict_ehull_Eg_t_B_G_del['Ehull_predict(eV/atom)']==1) & \n",
    "                         (df_D1_predict_ehull_Eg_t_B_G_del['Bg_predict(eV)']*1.21+0.36>=1) &\n",
    "                         (df_D1_predict_ehull_Eg_t_B_G_del['Bg_predict(eV)']*1.21+0.36<=1.7) &\n",
    "                        (df_D1_predict_ehull_Eg_t_B_G_del['t']<4.18) &\n",
    "                         (df_D1_predict_ehull_Eg_t_B_G_del['t']>0) \n",
    "                         & (df_D1_predict_ehull_Eg_t_B_G_del['G_predict(GPa)']/df_D1_predict_ehull_Eg_t_B_G_del['B_predict(GPa)']<0.2)\n",
    "                         \n",
    "                        ].loc[:,['formula','Bg_predict(eV)']]\n",
    "df_D1_predict_ehull_Eg_t_B_G[df_D1_predict_ehull_Eg_t_B_G['formula']=='Cs2AgVBr6'].loc[:,['formula','Bg_predict(eV)']]\n",
    "df_D1_predict_ehull_Eg_t.to_excel('表数据/卤素双钙ehull带隙结构因子筛选结果16529条20230108.xlsx')\n",
    "df_D1_predict_ehull[df_D1_predict_ehull['Ehull_predict(eV/atom)']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086d142-1c64-4366-baa0-589677d58d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bandgap\n",
    "## 回归预测\n",
    "def predict_Bg_by_ANN(x_train,x_test,ANN_model):\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    y_predict = ANN_model.predict(x_test_scaled)\n",
    "    return pd.DataFrame(y_predict,columns=['Bg_predict(eV)'])\n",
    "df_D1_predict_Eg = pd.concat([df_predictSet,predict_Bg_by_ANN(X_D1_train,\n",
    "                        df_predictSet.loc[:,'MagpieData minimum Number':'MagpieData mode SpaceGroupNumber'],\n",
    "                         model_Bg_D1_B1_1_TL,\n",
    "                    )],axis=1)\n",
    "df_D1_screen_ehull_to_Bg = df_D1_predict_Eg[df_D1_predict_ehull['Ehull_predict(eV/atom)']==1][df_D1_predict_Eg[df_D1_predict_ehull['Ehull_predict(eV/atom)']==1]['Bg_predict(eV)']<2.5][df_D1_predict_Eg[df_D1_predict_ehull['Ehull_predict(eV/atom)']==1][df_D1_predict_Eg[df_D1_predict_ehull['Ehull_predict(eV/atom)']==1]['Bg_predict(eV)']<2.5]['Bg_predict(eV)']>0.1]\n",
    "df_D1_screen_ehull_to_Bg\n",
    "def drop_TrainData_from_predictData():\n",
    "    index_list = []\n",
    "    for index, formula in df_D1_screen_ehull_to_Bg['formula'].items():\n",
    "        if formula not in list(df_D1['pretty_formula']):\n",
    "            index_list.append(index)\n",
    "    return df_D1_screen_ehull_to_Bg.loc[index_list,:]\n",
    "                       \n",
    "drop_TrainData_from_predictData()    \n",
    "df_drop_TrainData_from_predictData = drop_TrainData_from_predictData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b8b4b-6eeb-47bd-9467-859442fba70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#structure\n",
    "def screen_by_ToleranceFactor(df):\n",
    " \n",
    "    import re\n",
    "    import math\n",
    "    import pandas as pd\n",
    "    from mendeleev import element\n",
    "    \n",
    "    def numElement(df):\n",
    "        \n",
    "        element_list = []\n",
    "        for formu in df['formula']:\n",
    "            div = re.compile(r\"([A-Z][a-z])|([A-Z])|(\\(.*\\))\")\n",
    "            ele_list = div.findall(formu)\n",
    "            for el in ele_list:\n",
    "                for e in el:\n",
    "                    if e!='' and e not in ele_list:\n",
    "                        element_list.append(e)\n",
    "        element_list_100 = set(element_list)\n",
    "\n",
    "        return element_list_100\n",
    "    element_list = numElement(df=df)\n",
    "\n",
    "    df_single_ele_pre = pd.DataFrame(list(element_list),columns=['element'])\n",
    "\n",
    "    def getFeatures(df,to_path):\n",
    "        #导入本地特征表\n",
    "        path1 = r\"表数据/Supplementary Data 1-Elemental Properties of Atoms.csv\"\n",
    "        df_elem_proper = pd.read_csv(path1)\n",
    "        df_elem_proper_t = df_elem_proper.T\n",
    "        df_elem_proper_t.columns = list(df_elem_proper.symbol)\n",
    "\n",
    "        #批量创建空数组\n",
    "        feature_list_name_all = ['IR']\n",
    "        for var in feature_list_name_all:\n",
    "            # 为每个变量名创建一个空列表\n",
    "            # 例如：EN_pauling = []\n",
    "            globals()[var] = []\n",
    "        for index, elem in df.iloc[:, 0].iteritems():\n",
    "            IR.append(df_elem_proper_t.loc[\"ionic_radius\", elem])#离子半径\n",
    "        # 复制一份原输入的df\n",
    "        column1 = [0]\n",
    "        df_copy = df.iloc[:, column1]\n",
    "        feature_list_all = []\n",
    "        for var in feature_list_name_all:\n",
    "            feature_list_all.append(globals()[var])\n",
    "        for index, name in enumerate(feature_list_name_all):  # 遍历数组返回索引及数值\n",
    "\n",
    "            df_copy[name] = feature_list_all[index]\n",
    "    #     df_copy.to_excel(to_path)\n",
    "        display(df_copy)\n",
    "        print(\"成功！\")\n",
    "        return df_copy\n",
    "        # path = r'D:\\稀贵金属机器学习\\单元素.csv'\n",
    "\n",
    "    df_physical_single_pre=getFeatures(df_single_ele_pre, '')\n",
    "\n",
    "    def createPhysicalFeatures(df_train,df_physical_single,to_excel_path):\n",
    "\n",
    "        df_train_formula = df_train['formula'] \n",
    "        #元素物理特征生成表中用第一列元素名作为index\n",
    "        df_physical_single = df_physical_single.set_index('element')\n",
    "        # 匹配元素和原子个数的正则表达式\n",
    "        element_and_count_pattern = r\"([A-Z]{1}[a-z]*)(\\d*)\"\n",
    "        A,B,X = [],[],[]\n",
    "        A_count,B_count,X_count = [],[],[]\n",
    "        for key,value in df_train_formula.iteritems():\n",
    "            # 使用正则表达式查找化学式中的所有元素和原子个数\n",
    "            element_and_count_list = re.findall(element_and_count_pattern, value)\n",
    "            print(element_and_count_list)\n",
    "            A.append(element_and_count_list[0][0])\n",
    "            B.append(element_and_count_list[1][0])\n",
    "            X.append(element_and_count_list[2][0])\n",
    "            A_count.append(element_and_count_list[0][1])\n",
    "            B_count.append(element_and_count_list[1][1])\n",
    "            X_count.append(element_and_count_list[2][1])\n",
    "        # 给原子个数为空字符串的元素赋值为1\n",
    "        for i in range(len(A_count)):\n",
    "            A_count[i]=1 if A_count[i]=='' else A_count[i]\n",
    "            B_count[i]=1 if B_count[i]=='' else B_count[i]\n",
    "            X_count[i]=1 if X_count[i]=='' else X_count[i]\n",
    "        #生成物理特征，不考虑原子个数\n",
    "        #\n",
    "\n",
    "        for key,value in df_physical_single.iteritems():\n",
    "            A_feat = value[A].reset_index(drop=True)\n",
    "            B_feat = value[B].reset_index(drop=True)\n",
    "            X_feat = value[X].reset_index(drop=True)\n",
    "            sub_A_X = A_feat - X_feat\n",
    "            sub_B_X = B_feat - X_feat\n",
    "            sub_A_B = A_feat - B_feat\n",
    "            if key == 'IR':\n",
    "                #容忍因子 公式 t = (rA+rX)/根号2(rB+rX)\n",
    "                # 新容忍因子\n",
    "                new_t = (X_feat/B_feat)+(A_feat/B_feat)/((A_feat/B_feat).apply(np.log))-1\n",
    "                t = (A_feat+X_feat)/((2**(1/2))*(B_feat+X_feat))\n",
    "                # 八面体因子 公式 u = rB/rX\n",
    "                u = B_feat/X_feat\n",
    "                # 当前特征df\n",
    "                df_current_feat = pd.DataFrame({str('new_t'):new_t,str('t'):t,str('u'):u,str(key+'B-X'):sub_B_X,\n",
    "                                                str(key+'A-X'):sub_A_X,str(key+'A-B'):sub_A_B,\n",
    "                                                str(key+'_X'):X_feat,str(key+'_B'):B_feat,\n",
    "                                                str(key+'_A'):A_feat\n",
    "                                        })\n",
    "            else:\n",
    "                # 当前特征df\n",
    "                df_current_feat = pd.DataFrame({str(key+'B-X'):sub_B_X,\n",
    "                                                str(key+'A-X'):sub_A_X,str(key+'A-B'):sub_A_B,\n",
    "                                                str(key+'_X'):X_feat,str(key+'_B'):B_feat,\n",
    "                                                str(key+'_A'):A_feat\n",
    "                                            })\n",
    "            #拼接当前特征df到原表\n",
    "            df_train_new = pd.concat([df_train,df_current_feat],axis=1)\n",
    "        display(df_train_new)\n",
    "    #     df_train.to_excel(to_excel_path)\n",
    "        return df_train_new\n",
    "\n",
    "    df_t_pre = createPhysicalFeatures(df,df_physical_single_pre,'')      \n",
    "    return df_t_pre  \n",
    "\n",
    "# df_screen_by_ToleranceFactor = screen_by_ToleranceFactor(df_drop_TrainData_from_predictData.reset_index(drop=True))\n",
    "df_screen_by_ToleranceFactor[df_screen_by_ToleranceFactor['t']>0][df_screen_by_ToleranceFactor[df_screen_by_ToleranceFactor['t']>0]['t']<4.18]\n",
    "df_screen_by_ToleranceFactor[df_screen_by_ToleranceFactor['t']>0][df_screen_by_ToleranceFactor[df_screen_by_ToleranceFactor['t']>0]['t']<4.18]['Bg_predict(eV)']*1.21+0.36<\n",
    "display(df_D1_predict_Eg[df_D1_predict_Eg['formula']=='Cs2AgSbBr6']['Bg_predict(eV)'])\n",
    "display(df_D1[df_D1['full_formula']=='K2Ag1Sb1Cl6']['band_gap'])\n",
    "df_D1_screen_ehull_to_Bg[df_D1_screen_ehull_to_Bg['formula']=='Cs2KCoBr6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17cb93b-ae58-4f8a-b5fa-c0e7296d25b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee14fb-d5ac-4f85-8a64-7faff49f53a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML2]",
   "language": "python",
   "name": "conda-env-ML2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
